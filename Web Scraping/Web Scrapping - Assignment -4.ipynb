{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d0f95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import time \n",
    "from selenium.common.exceptions import NoSuchElementException     \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18309868",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url\n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: \n",
    "A)Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b49b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = driver.find_element(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in table.find_elements(By.TAG_NAME, 'tr')[1:4]:\n",
    "    columns = row.find_elements(By.TAG_NAME, 'td')\n",
    "    rank = columns[0].text\n",
    "    name = columns[1].text\n",
    "    artist = columns[2].text\n",
    "    upload_date = columns[3].text\n",
    "    views = columns[4].text\n",
    "\n",
    "    print(f\"Rank: {rank}, Name: {name}, Artist: {artist}, Upload date: {upload_date}, Views: {views}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10f1d6",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cad7c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize Chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "try:\n",
    "\n",
    "    fixture_link = WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.LINK_TEXT, \"International\"))\n",
    "    )\n",
    "    fixture_link.click()\n",
    "\n",
    "   \n",
    "    fixtures_link = WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.LINK_TEXT, \"Fixtures\"))\n",
    "    )\n",
    "    fixtures_link.click()\n",
    "\n",
    "\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.CLASS_NAME, \"fixture__format-strip\"))\n",
    "    )\n",
    "\n",
    "   \n",
    "    fixtures = driver.find_elements_by_class_name(\"fixture__format-strip\")\n",
    "\n",
    "    for fixture in fixtures:\n",
    "      \n",
    "        series = fixture.find_element_by_class_name(\"fixture__format-strip-replace\").text.strip()\n",
    "        place = fixture.find_element_by_class_name(\"fixture__description\").text.strip()\n",
    "        date = fixture.find_element_by_class_name(\"fixture__datetime\").text.split(',')[0].strip()\n",
    "        time = fixture.find_element_by_class_name(\"fixture__datetime\").text.split(',')[1].strip()\n",
    "\n",
    "        print(\"Series:\", series)\n",
    "        print(\"Place:\", place)\n",
    "        print(\"Date:\", date)\n",
    "        print(\"Time:\", time)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "finally:\n",
    "  \n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db0b5a",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "    \n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    fixture_link = WebDriverWait(driver, 10).until(\n",
    "    economy_link = driver.find_elements((By.XPATH,\"//div[@id='top']/div[2]/div[2]/div[1]/div/div[2]/div[2]/ul/li[3]/a\"))\n",
    "    )\n",
    "    economy_link.click()\n",
    "\n",
    "    fixture_link = WebDriverWait(driver, 10).until(\n",
    "    gdp_link = driver.find_elements((By.XPATH,\"//div[@id='navbar']/div[2]/div/a[3]\"))\n",
    "    )\n",
    "    gdp_link.click()\n",
    "\n",
    "        # Find the GDP table\n",
    "    table = driver.find_elements(By.ID,\"table_id\")\n",
    "\n",
    "        \n",
    "    rows = table.findall_elements(BY.TAG_NAME,\"tr\")\n",
    "\n",
    "    for row in rows[1:]:  \n",
    "        columns = row.find_elements(BY.TAG_NAME,\"td\")\n",
    "        rank = columns[0].text\n",
    "        state = columns[1].text\n",
    "        gdp_18_19 = columns[2].text\n",
    "        gdp_19_20 = columns[3].text\n",
    "        share_18_19 = columns[4].text\n",
    "        gdp_billion = columns[5].text\n",
    "\n",
    "        print(\"Rank:\", rank)\n",
    "        print(\"State:\", state)\n",
    "        print(\"GSDP(18-19) - at current prices:\", gdp_18_19)\n",
    "        print(\"GSDP(19-20) - at current prices:\", gdp_19_20)\n",
    "        print(\"Share(18-19):\", share_18_19)\n",
    "        print(\"GDP($ billion):\", gdp_billion)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f44c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ed044",
   "metadata": {},
   "source": [
    "# 4.Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "def scrape_github_trending():\n",
    "   \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://github.com/\")\n",
    "    \n",
    "    try:\n",
    "       \n",
    "        explore_menu = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//summary[contains(text(),'Explore')]\")))\n",
    "        explore_menu.click()\n",
    "\n",
    "      \n",
    "        trending_option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.LINK_TEXT, \"Trending\")))\n",
    "        trending_option.click()\n",
    "\n",
    "       \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//article[@class='Box-row']\")))\n",
    "        \n",
    "       \n",
    "        repositories = driver.find_elements_by_xpath(\"//article[@class='Box-row']\")\n",
    "\n",
    "        \n",
    "        for repo in repositories:\n",
    "            title = repo.find_element_by_css_selector(\"h1.h3\").text\n",
    "            description = repo.find_element_by_css_selector(\"p\").text\n",
    "            language = repo.find_element_by_css_selector(\"span[itemprop='programmingLanguage']\").text\n",
    "            contributors_count = len(repo.find_elements_by_css_selector(\"a.muted-link[href$='/graphs/contributors']\"))\n",
    "\n",
    "            print(\"Repository Title:\", title)\n",
    "            print(\"Description:\", description)\n",
    "            print(\"Language:\", language)\n",
    "            print(\"Contributors Count:\", contributors_count)\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "\n",
    "\n",
    "\n",
    "scrape_github_trending()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7c28e",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the\n",
    "following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20282b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5dea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "def scrape_billboard_top_100(url):\n",
    "   \n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        \n",
    "        charts_option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//a[@class=\"header__main-link header__main-link--charts\"]')))\n",
    "        charts_option.click()\n",
    "        \n",
    "       \n",
    "        hot_100_link = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//a[@class=\"charts-landing__link\"]')))\n",
    "        hot_100_link.click()\n",
    "        \n",
    "        \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//table[@class=\"chart-list chart-details__left-rail\"]')))\n",
    "        \n",
    "        \n",
    "        song_details = []\n",
    "        rows = driver.find_elements_by_xpath('//div[@class=\"chart-list-item\"]')\n",
    "        for row in rows:\n",
    "            song_name = row.find_element_by_xpath('.//span[@class=\"chart-list-item__title-text\"]').text\n",
    "            artist_name = row.find_element_by_xpath('.//div[@class=\"chart-list-item__artist\"]').text\n",
    "            last_week_rank = row.find_element_by_xpath('.//div[@class=\"chart-list-item__last-week\"]').text\n",
    "            peak_rank = row.find_element_by_xpath('.//div[@class=\"chart-list-item__weeks-at-one\"]').text\n",
    "            weeks_on_board = row.find_element_by_xpath('.//div[@class=\"chart-list-item__weeks-on-chart\"]').text\n",
    "            song_details.append({\n",
    "                \"Song Name\": song_name,\n",
    "                \"Artist Name\": artist_name,\n",
    "                \"Last Week Rank\": last_week_rank,\n",
    "                \"Peak Rank\": peak_rank,\n",
    "                \"Weeks on Board\": weeks_on_board\n",
    "            })\n",
    "        \n",
    "        return song_details\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    \n",
    "\n",
    "top_100_songs = scrape_billboard_top_100(url)\n",
    "if top_100_songs:\n",
    "    for idx, song in enumerate(top_100_songs, start=1):\n",
    "        print(f\"Song {idx}:\")\n",
    "        for key, value in song.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925670f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda2bca",
   "metadata": {},
   "source": [
    "#  6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    "Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81168bd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "def scrape_novel_details(url):\n",
    "    \n",
    "    driver = webdriver.Chrome()  \n",
    "    \n",
    "    try:\n",
    "       \n",
    "        driver.get(url)\n",
    "        \n",
    "        \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table.full-width\")))\n",
    "        \n",
    "        \n",
    "        rows = driver.find_elements_by_css_selector(\"table.full-width tbody tr\")\n",
    "        \n",
    "       \n",
    "        book_names = []\n",
    "        author_names = []\n",
    "        volumes_sold = []\n",
    "        publishers = []\n",
    "        genres = []\n",
    "        \n",
    "        \n",
    "        for row in rows:\n",
    "            cells = row.find_elements_by_tag_name(\"td\")\n",
    "            book_names.append(cells[1].text)\n",
    "            author_names.append(cells[2].text)\n",
    "            volumes_sold.append(cells[3].text)\n",
    "            publishers.append(cells[4].text)\n",
    "            genres.append(cells[5].text)\n",
    "        \n",
    "        \n",
    "        for i in range(len(book_names)):\n",
    "            print(\"Book Name:\", book_names[i])\n",
    "            print(\"Author Name:\", author_names[i])\n",
    "            print(\"Volumes Sold:\", volumes_sold[i])\n",
    "            print(\"Publisher:\", publishers[i])\n",
    "            print(\"Genre:\", genres[i])\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "   \n",
    "scrape_novel_details(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b62eba",
   "metadata": {},
   "source": [
    "# 7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524c386",
   "metadata": {},
   "source": [
    "# ERROR 404 on website "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd37831",
   "metadata": {},
   "source": [
    "# 8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You\n",
    "have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69dce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "def scrape_dataset_details(url):\n",
    "    \n",
    "    driver = webdriver.Chrome()  \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "       \n",
    "        view_all_link = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, \"View ALL datasets\")))\n",
    "        view_all_link.click()\n",
    "        \n",
    "        \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//table[@border='1']\")))\n",
    "        \n",
    "        \n",
    "        rows = driver.find_elements_by_xpath(\"//table[@border='1']/tbody/tr\")[1:]  # Exclude header row\n",
    "        \n",
    "        \n",
    "        for row in rows:\n",
    "            cells = row.find_elements_by_tag_name(\"td\")\n",
    "            dataset_name = cells[0].text\n",
    "            data_type = cells[1].text\n",
    "            task = cells[2].text\n",
    "            attribute_type = cells[3].text\n",
    "            num_instances = cells[4].text\n",
    "            num_attributes = cells[5].text\n",
    "            year = cells[6].text\n",
    "            \n",
    "            print(\"Dataset Name:\", dataset_name)\n",
    "            print(\"Data Type:\", data_type)\n",
    "            print(\"Task:\", task)\n",
    "            print(\"Attribute Type:\", attribute_type)\n",
    "            print(\"No of Instances:\", num_instances)\n",
    "            print(\"No of Attributes:\", num_attributes)\n",
    "            print(\"Year:\", year)\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        \n",
    "scrape_dataset_details(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8d951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
